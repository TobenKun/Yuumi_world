---
{"dg-publish":true,"permalink":"/CODING/Python/001.41 이번엔 학습이다/","noteIcon":"2"}
---

실제 신경망은 매개변수가 수천 수만이고 딥러닝은 수억인데
그 많은 매개변수 직접 다 정할겨? 불가능하지 아무래도

그러니까 이번에는 데이터로부터 매개변수의 값을 정하는 방법에 대해 배워보자

## 기계학습과 신경망(딥러닝)
### 기계학습
- 모아진 데이터로부터 규칙을 찾아내는 역할을 기계가 담당
- 입력 데이터에서 본질적인 데이터를 추출하는 변환기는 사람이 설계

### 신경망(딥러닝)
- 데이터를 있는 그대로 학습(즉 중요한 특징까지도 기계가 스스로 학습)
- 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 학습할 수 있음
- 종단간 기계학습(end-to-end machine learning)
### 🔥 정리: 기계학습 vs 딥러닝 비교

| 비교 항목        | 기계학습 (Machine Learning) | 딥러닝 (Deep Learning)       |
| ------------ | ----------------------- | ------------------------- |
| **특징 추출**    | 사람이 직접 설계               | 신경망이 자동으로 학습              |
| **알고리즘**     | SVM, 랜덤 포레스트 등          | CNN, RNN, Transformer 등   |
| **데이터 필요량**  | 적은 데이터로도 가능             | 많은 데이터가 필요                |
| **연산 성능**    | CPU로 가능                 | GPU 필요                    |
| **해석 가능성**   | 비교적 명확함                 | 블랙박스 (왜 그렇게 결정했는지 알기 어려움) |
| **적용 가능 분야** | 일반적인 데이터 분석             | 이미지, 음성, 자연어 처리 등         |

### 데이터 취급 주의!
기계학습 문제는 데이터를 훈련 데이터, 시험 데이터로 나눠서 학습과 실험을 수행한다.
처음 보는 데이터로도 문제를 올바르게 풀어내는 능력이 최종 목표.

데이터셋 하나로 학습과 평가를 하면 한 데이터셋에만 지나치게 최적화된 오버피팅 상태가 될 수 있다.


## 손실함수
여기서부터 수학 빡세보인다 집중!

신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현한다.
신경망도 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다.
이때 사용되는 지표가 손실 함수(loss function) 
일반적으로 오차제곱합과 교차 엔트로피 오차를 사용

> [!warning] 손실함수
> 손실함수는 신경망 성능의 '나쁨'을 나타내는 지표.
> 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄.
> 나쁨을 지표로 삼는게 어색하긴 한데 본질적으로 수행하는 일은 같음


### 오차제곱합
가장 많이 쓰이는 손실 함수인 오차제곱합`sum of squares for error, SSE`
어우 라텍스 문법으로 적는것도 어렵다..
$$E=\frac{1}{2}\sum_k(y_k-t_k)^2$$
$y_k$는 신경망의 출력(신경망이 추정한 값)
$t_k$는 정답 레이블 (한 원소만 1이고 나머지는 0인 원-핫 인코딩)
$k$는 데이터의 차원 수를 나타낸다.

```python
def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2) # 각 요소를 제곱 후 더함
```

함수 결과가 더 작은값이 정답에 더 가까울 것으로 판단할 수 있다.

##### 번외: 제곱을 쓰는 이유
단순히 오차 합을 사용하면 양수와 음수가 서로 상쇄되기 때문
양수화를 위해서 절댓값이나 제곱을 사용해야함 

그런데 절댓값을 사용하면 미분이 불가능한 지점이 생기기 때문에 최적화에 문제가 생긴다고 함!
그래서 제곱합을 쓴답니다~~

앞에 뜬금없이 $\frac{1}{2}$가 붙는 이유도 미분할때 깔끔해서...라고하네요

### 교차 엔트로피 오차
cross entropy error, CEE
$$E=-\sum_kt_k\log{y_k}$$
$log$는 밑이 $e$인 자연로그 $log_e$
$y_k$는 신경망의 출력
$t_k$는 정답 레이블 (원-핫 인코딩)

$t_k$가 원-핫 인코딩이고 로그 앞에 곱해지기 때문에 실질적으로
정답일때의 추정($t_k$가 1일때의 $y_k$)의 자연로그를 계산하는 식이다.

자연로그 $y=\log{x}$ 그래프는 x가1일때 y가 0이 되고,
x가 0에 가까워 질수록 y의 값이 점점 작아진다.

즉 교차 엔트로피 오차의 결과값은 정답일때의 출력이 1에 가까울수록 작은 값,
출력이 0에 가까워질수록 큰 값을 가지게 된다. (그러니까 오차지!)

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

y와 t는 넘파이 배열
아주 작은 값인 델타를 모든 요소에 더해준다.

##### 왜더함?
log(0)은 정의되지 않았고, 함수에 넣으면  devide by zero 경고와 함께 -inf값을 리턴한다.
->
컴퓨터가 로그 값을 계산할 때 특정 방법이 1/x 꼴을 사용한다고 함
로그를 미분하면 $\frac{1}{x}$ 꼴이 되기 때문... 어렵구나 어려워
이런 문제를 막기 위해 델타를 더해준다.